{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "# Pipelining\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE=42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge is to predict the CTR (click through rate). \n",
    "The original Kaggle challenge although focus on Fraud but we can still proceed with similar fashion of approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers the core logic behind the code. Majority of the time was spent on structuring the ETL \n",
    "(outside of this notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 7 features in total, where 5 features are categorical and the other 2 as date features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['ip', 'app', 'device', 'os', 'channel']\n",
    "datetime_features = ['click_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`attributed_time` is defined as :\n",
    "```\n",
    "if user download the app for after clicking an ad, this is the time of the app download\n",
    "```\n",
    "\n",
    "This would therefore make more sense to exclude `attributed_time` as training feature, we could use it for sanity\n",
    "check instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable types, will be a schema for actual external database\n",
    "dtypes = {\n",
    "    'ip': 'uint32',\n",
    "    'app': 'uint16',\n",
    "    'device': 'uint16',\n",
    "    'os': 'uint16',\n",
    "    'channel': 'uint16',\n",
    "    'is_attributed': 'uint8',\n",
    "    'click_id': 'uint32'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the limiting size of machine used, total data points used for both train and test are a sub sample of the original\n",
    "dataset.  \n",
    "\n",
    "In the use case of actual large data consumption scenarios, distributed framework such as Spark will serve as a \n",
    "better tool for parallel batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = pd.read_csv(data_path + '/train/train.csv', \n",
    "                           dtype=dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced dataset can be countered using two general approaches:\n",
    "    \n",
    "1. Oversampling  \n",
    "2. Undersampling  \n",
    "\n",
    "(1) Oversampling strategies:  \n",
    "    - Use SMOTE-NC to synthesize more data points using the attributes of nearest-neighbours  \n",
    "    - Tune scale_pos_weight that increases the weight for minority class\n",
    "    \n",
    "(2) Undersampling strategies:  \n",
    "    - Reduce the negative sample population to get closer of that of the positive ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current sampling strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, the amount of data points are huge - 184,903,891 (about 184 million) and positive sample only\n",
    "consists of 0.2% of the total population.\n",
    "\n",
    "In order to let the model pick up more patterns of the positive sample, all positive samples are kept while we \n",
    "undersampled the negative samples randomly.\n",
    "\n",
    "Total number of data points is predefined, minus all positive (minority)sample population which will then be the \n",
    "negative (majority) sample population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_target_data_points = 1500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label = 'is_attributed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    184447044\n",
       " 1       456846\n",
       " Name: is_attributed, dtype: int64,\n",
       " 0    0.997529\n",
       " 1    0.002471\n",
       " Name: is_attributed, dtype: float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full[y_label].value_counts(0), data_full[y_label].value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all positive sample\n",
    "data_pos_only = data_full[data_full[y_label] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pos_points = data_pos_only.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_neg_sampled = data_full[data_full[y_label] == 0].sample(n=(num_target_data_points - num_pos_points)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form sampled new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sampled = pd.concat([data_neg_sampled, data_pos_only], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data_sampled[y_label]\n",
    "features = data_sampled.drop(columns=[y_label, 'attributed_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500000 data points with 8 features.\n"
     ]
    }
   ],
   "source": [
    "num_data_points, num_features = data_sampled.shape\n",
    "print(f\"{num_data_points} data points with {num_features} features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    1043154\n",
       " 1     456846\n",
       " Name: is_attributed, dtype: int64,\n",
       " 0    0.695436\n",
       " 1    0.304564\n",
       " Name: is_attributed, dtype: float64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.value_counts(0), labels.value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>attributed_time</th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>127088</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>178</td>\n",
       "      <td>2017-11-09 13:47:04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84671</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>328</td>\n",
       "      <td>2017-11-07 03:59:23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>123994</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>134</td>\n",
       "      <td>2017-11-07 00:09:19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106824</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>448</td>\n",
       "      <td>2017-11-09 11:00:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94729</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>173</td>\n",
       "      <td>2017-11-08 14:21:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip  app  device  os  channel           click_time attributed_time  \\\n",
       "0  127088   12       1  17      178  2017-11-09 13:47:04             NaN   \n",
       "1   84671   12       1  16      328  2017-11-07 03:59:23             NaN   \n",
       "2  123994    9       1  15      134  2017-11-07 00:09:19             NaN   \n",
       "3  106824    9       1  22      448  2017-11-09 11:00:24             NaN   \n",
       "4   94729    3       1  53      173  2017-11-08 14:21:01             NaN   \n",
       "\n",
       "   is_attributed  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sampled.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test (label name is pre-defined)\n",
    "features, labels = data_sampled.drop(columns=[y_label]), data_sampled[y_label]\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(features, labels,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=RANDOM_STATE,\n",
    "                                                    stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ip have 0 nulls, 0.0 %.\n",
      "app have 0 nulls, 0.0 %.\n",
      "device have 0 nulls, 0.0 %.\n",
      "os have 0 nulls, 0.0 %.\n",
      "channel have 0 nulls, 0.0 %.\n",
      "click_time have 0 nulls, 0.0 %.\n",
      "attributed_time have 834523 nulls, 55.634866666666674 %.\n"
     ]
    }
   ],
   "source": [
    "for col in train_X.columns:\n",
    "    nan_abs_val = train_X[col].isna().sum()\n",
    "    nan_percent_val = nan_abs_val / num_data_points * 100\n",
    "    print(f\"{col} have {nan_abs_val} nulls, {nan_percent_val} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Unique (only for categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ip have unique number of categories: 222624, 14.8% .\n",
      "app have unique number of categories: 346, 0.0231% .\n",
      "device have unique number of categories: 1755, 0.117% .\n",
      "os have unique number of categories: 201, 0.0134% .\n",
      "channel have unique number of categories: 179, 0.0119% .\n"
     ]
    }
   ],
   "source": [
    "for col in categorical_features:\n",
    "    num_cat_uniq = train_X[col].nunique()\n",
    "    percent_cat_uniq = num_cat_uniq / num_data_points * 100\n",
    "    print(f\"{col} have unique number of categories: {num_cat_uniq}, {percent_cat_uniq:.3g}% .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check categorical encoded range and common elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "For ip, min is 1, max is 364777\n",
      "Top 10 most common ip categories with frequency:\n",
      "[   (5348, 7472),\n",
      "    (5314, 6872),\n",
      "    (73487, 4381),\n",
      "    (73516, 4255),\n",
      "    (53454, 2377),\n",
      "    (26995, 2086),\n",
      "    (95766, 2048),\n",
      "    (114276, 2041),\n",
      "    (105475, 1634),\n",
      "    (17149, 1555)]\n",
      "==============================\n",
      "For app, min is 0, max is 768\n",
      "Top 10 most common app categories with frequency:\n",
      "[   (3, 161335),\n",
      "    (12, 111104),\n",
      "    (19, 109574),\n",
      "    (2, 103113),\n",
      "    (9, 89327),\n",
      "    (18, 77639),\n",
      "    (15, 74852),\n",
      "    (35, 51379),\n",
      "    (14, 47007),\n",
      "    (29, 34801)]\n",
      "==============================\n",
      "For device, min is 0, max is 4223\n",
      "Top 10 most common device categories with frequency:\n",
      "[   (1, 1032639),\n",
      "    (0, 85722),\n",
      "    (2, 38316),\n",
      "    (3032, 3112),\n",
      "    (6, 2387),\n",
      "    (40, 2169),\n",
      "    (16, 1766),\n",
      "    (3543, 1240),\n",
      "    (18, 1178),\n",
      "    (21, 948)]\n",
      "==============================\n",
      "For os, min is 0, max is 866\n",
      "Top 10 most common os categories with frequency:\n",
      "[   (19, 263336),\n",
      "    (13, 228949),\n",
      "    (17, 52940),\n",
      "    (18, 49882),\n",
      "    (22, 43947),\n",
      "    (24, 39512),\n",
      "    (0, 31873),\n",
      "    (10, 28849),\n",
      "    (8, 28500),\n",
      "    (6, 25434)]\n",
      "==============================\n",
      "For channel, min is 0, max is 498\n",
      "Top 10 most common channel categories with frequency:\n",
      "[   (213, 109774),\n",
      "    (280, 71259),\n",
      "    (113, 46534),\n",
      "    (107, 41551),\n",
      "    (245, 41068),\n",
      "    (101, 36068),\n",
      "    (21, 35027),\n",
      "    (477, 33564),\n",
      "    (134, 29675),\n",
      "    (259, 28000)]\n"
     ]
    }
   ],
   "source": [
    "for col in categorical_features:\n",
    "    print('='*30)\n",
    "    print(f\"For {col}, min is {train_X[col].min()}, max is {train_X[col].max()}\")\n",
    "    c = Counter(train_X[col]).most_common(10)\n",
    "    top_10_common_elements = sorted(c, key=lambda x:x[1], reverse=True)\n",
    "    \n",
    "    print(f\"Top 10 most common {col} categories with frequency:\") \n",
    "    pp.pprint(top_10_common_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.695436\n",
       "1    0.304564\n",
       "Name: is_attributed, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.value_counts(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features would be generated via sklearn transformer to retain any states and for clearer code structural consturction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractorTransformer(TransformerMixin):\n",
    "\n",
    "    def __init__(self, feature_list):\n",
    "        self.feature_list = feature_list\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        # Return selected features from dataframe\n",
    "        return input_df[self.feature_list]\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common and simplest way to deal with categorical features is one-hot encoding. \n",
    "However, the dimensions to deal with here are quite high.\n",
    "\n",
    "We can reduce the cardinality by picking only top n most frequent cateogries and group the rest into a single category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalReduceTransformer(TransformerMixin):\n",
    "\n",
    "    def __init__(self, config={}):\n",
    "\n",
    "        self.top_n = config.get('top_n', None)\n",
    "        self.feature_list = config.get('feature_list', None)\n",
    "        self.default_column_names = config.get('default_column_names', None)\n",
    "\n",
    "        self.one_hot_columns = None\n",
    "        self.all_columns = None\n",
    "        self.top_n_cats = {}\n",
    "\n",
    "    def transform(self, input_df, **transform_params):\n",
    "\n",
    "        # Check if the input is dataframe, if not convert to dataframe with set columns\n",
    "        if not isinstance(input_df, pd.DataFrame):\n",
    "            input_df = pd.DataFrame(input_df, columns=self.default_column_names)\n",
    "\n",
    "        if self.feature_list is not None:\n",
    "            feat_list = self.feature_list\n",
    "        else:\n",
    "            feat_list = input_df.columns\n",
    "\n",
    "        one_hot_cols = []\n",
    "        # Apply one_hot coding for all features in feature list\n",
    "        for col in feat_list:\n",
    "            input_df[col] = input_df[col].map(str)\n",
    "\n",
    "            if self.top_n_cats:\n",
    "                # New / unknown value will be treated as minority\n",
    "                # Top n most frequent categories and None values retained respective encoding\n",
    "                input_df.loc[~input_df[col].isin(self.top_n_cats[col]), col] = '-1'\n",
    "\n",
    "            # Get the one_hot coding\n",
    "            one_hot_df = pd.get_dummies(input_df[col], prefix=col)\n",
    "\n",
    "            # Drop original feature from dataset\n",
    "            input_df = input_df.drop(columns=[col])\n",
    "\n",
    "            # Add one hot coding instead of original feature\n",
    "            input_df = pd.concat([input_df, one_hot_df], axis=1)\n",
    "\n",
    "            # Keep track of one_hot columns in train set\n",
    "            one_hot_cols.extend(one_hot_df.columns)\n",
    "            \n",
    "        # If transformer has stored state (fit was used), assign unknown / new category with 0\n",
    "        missing_columns = list(set(self.one_hot_columns) - set(one_hot_cols))\n",
    "\n",
    "        # Assign any missing columns as all zeros\n",
    "        input_df[missing_columns] = 0\n",
    "\n",
    "        # Check the order of columns are the same\n",
    "        input_df = input_df[list(self.all_columns) + list(self.one_hot_columns)]\n",
    "\n",
    "        return input_df\n",
    "\n",
    "    def fit(self, input_df, *_):\n",
    "\n",
    "        # Check if the input is dataframe\n",
    "        if not isinstance(input_df, pd.DataFrame):\n",
    "            input_df = pd.DataFrame(input_df, columns=self.default_column_names)\n",
    "\n",
    "        if self.feature_list is not None:\n",
    "            feat_list = self.feature_list\n",
    "        else:\n",
    "            feat_list = input_df.columns\n",
    "\n",
    "        one_hot_cols = []\n",
    "\n",
    "        # Apply one hot coding for all features in feature list\n",
    "        for col in feat_list:\n",
    "            input_df[col] = input_df[col].map(str)\n",
    "\n",
    "            if self.top_n is not None:\n",
    "                # Get top n most frequent categories, replace the minority as single class\n",
    "                cat_counter = Counter(input_df[col]).most_common(self.top_n)\n",
    "                self.top_n_cats[col] = set([c[0] for c in cat_counter] + ['nan'])\n",
    "                input_df.loc[~input_df[col].isin(self.top_n_cats[col]), col] = '-1'\n",
    "\n",
    "            # Get the one_hot coding\n",
    "            one_hot_df = pd.get_dummies(input_df[col], prefix=col)\n",
    "\n",
    "            # Drop original feature from dataset\n",
    "            input_df = input_df.drop(columns=[col])\n",
    "\n",
    "            # Keep track of one_hot columns in train set\n",
    "            one_hot_cols.extend(one_hot_df.columns)\n",
    "\n",
    "        self.one_hot_columns = one_hot_cols\n",
    "        self.all_columns = input_df.columns\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_feature_list(self):\n",
    "        return list(self.all_columns) + list(self.one_hot_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datetime feature can be generated by breaking down to individual temporal dimension i.e hour, minutes etc.\n",
    "\n",
    "These can then be further break down into combination of sin and cos - cyclic representation of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_mappings = {\n",
    "    'month': 12,\n",
    "    'day': 31,\n",
    "    'weekday': 7,\n",
    "    'hour': 24,\n",
    "    'minute': 60,\n",
    "    'second': 60\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateTimeTransformer(TransformerMixin):\n",
    "\n",
    "    def __init__(self, feature_list=None):\n",
    "        self.feature_list = feature_list\n",
    "\n",
    "    def transform(self, df):\n",
    "        # Only collect transformed feature columns\n",
    "        dt_feats = pd.DataFrame()\n",
    "\n",
    "        for col in df.columns:\n",
    "            # Extract datetime series\n",
    "            dt_series = df[col]\n",
    "\n",
    "            if isinstance(dt_series, pd.Series):\n",
    "                try:\n",
    "                    dt_series = pd.to_datetime(dt_series)\n",
    "\n",
    "                    for dt_scale, dt_mval in dt_mappings.items():\n",
    "                        # Since sin-cos is represented in a circular fashion, (0,0) is never reached\n",
    "                        # This is hence used to indicate None\n",
    "                        conv_dt_series = 2*np.pi*getattr(dt_series.dt, dt_scale) / dt_mval\n",
    "                        dt_feats[f'{col}_sin_{dt_scale}'] = np.sin(conv_dt_series).fillna(0)\n",
    "                        dt_feats[f'{col}_cos_{dt_scale}'] = np.cos(conv_dt_series).fillna(0)\n",
    "\n",
    "                except Exception:\n",
    "                    raise ValueError(\"Invalid datetime object or string.\")\n",
    "            else:\n",
    "                raise ValueError(\"Input must be Pandas Series type.\")\n",
    "\n",
    "        return dt_feats\n",
    "\n",
    "    def fit(self, *_):\n",
    "        # This should be a stateless transformer\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_config = {\n",
    "    'category': {\n",
    "        'top_n': 5,\n",
    "        'feature_list': ['ip', 'app', 'device', 'os', 'channel'],\n",
    "    },\n",
    "    'datetime': ['click_time']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_config = features_config['category']\n",
    "datetime_feats = features_config['datetime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_pipeline = FeatureUnion([\n",
    "        ('category', Pipeline([\n",
    "            ('extract', FeatureExtractorTransformer(categorical_config['feature_list'])),\n",
    "            ('one_hot', CategoricalReduceTransformer(categorical_config))\n",
    "        ])),\n",
    "        ('datetime', Pipeline([\n",
    "            ('extract', FeatureExtractorTransformer(datetime_feats)),\n",
    "            ('datetime', DateTimeTransformer())\n",
    "        ]))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "class XGBoost:\n",
    "    def __init__(self, extra_grid_params={}):\n",
    "        self.name = 'xgboost'\n",
    "        self.model = XGBClassifier\n",
    "        self.grid_params = {\n",
    "            'clf__n_estimators': np.arange(100, 300, 100),  # number of trees\n",
    "            'clf__learning_rate': [0.1],\n",
    "            'clf__max_depth': np.arange(2, 8, 3),  # max number of levels in each decision tree,\n",
    "        }\n",
    "        self.grid_params.update(extra_grid_params)\n",
    "\n",
    "    def gen_model_grid_params(self):\n",
    "        model_content = {\n",
    "            'model': self.model,\n",
    "            'params': self.grid_params\n",
    "        }\n",
    "        return model_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_pipeline = XGBoost()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Flow Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_flow_pipeline = Pipeline([\n",
    "    ('features', preprocessor_pipeline),\n",
    "    ('clf', ml_pipeline.model())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_metric = 'roc_auc'\n",
    "n_jobs = 4\n",
    "n_inner_fold = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wchen/projects/adtrack_ctr/venv/lib/python3.7/site-packages/sklearn/model_selection/_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n",
      "/Users/wchen/projects/adtrack_ctr/venv/lib/python3.7/site-packages/ipykernel_launcher.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/wchen/projects/adtrack_ctr/venv/lib/python3.7/site-packages/pandas/core/indexing.py:1763: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n",
      "/Users/wchen/projects/adtrack_ctr/venv/lib/python3.7/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training took 559.76 seconds.\n"
     ]
    }
   ],
   "source": [
    "model_train_start = time.time()\n",
    "\n",
    "# Stratified K Fold validation as to maintain the imbalanced data distribution within folds\n",
    "k_fold = StratifiedKFold(n_splits=n_inner_fold, random_state=RANDOM_STATE)\n",
    "\n",
    "# Perform GridSearch Cross Validation    \n",
    "model = GridSearchCV(estimator=ml_flow_pipeline, \n",
    "                     param_grid=ml_pipeline.grid_params, \n",
    "                     scoring=score_metric, \n",
    "                     cv=k_fold, \n",
    "                     n_jobs=n_jobs)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "model_train_end = time.time()\n",
    "\n",
    "print(f\"Model training took {model_train_end-model_train_start:.5} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering we might be interested to use multiple metrics to measure the performance, we can create a class\n",
    "to accept the trained model and targeted performance metric. This can then be used to generate or save any \n",
    "visualisations (if needed). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, log_loss, recall_score\n",
    "\n",
    "available_metrics = {\n",
    "    'roc_auc_score': roc_auc_score,\n",
    "    'log_loss': log_loss,\n",
    "    'recall_score': recall_score\n",
    "}\n",
    "\n",
    "def get_feval(eval_metric):\n",
    "    if eval_metric not in available_metrics:\n",
    "        raise ValueError(f\"{eval_metric} is not available. Available metrics are {list(available_metrics.keys())}. \")\n",
    "\n",
    "    return available_metrics[eval_metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPerformance:\n",
    "    def __init__(self, model, metrics=None):\n",
    "        self.model = model\n",
    "        if metrics is None:\n",
    "            self.metrics = set()\n",
    "        else:\n",
    "            self.metrics = metrics\n",
    "\n",
    "    def set_metrics(self, metrics):\n",
    "        if isinstance(metrics, list):\n",
    "            metrics = set(metrics)\n",
    "\n",
    "        self.metrics = metrics\n",
    "\n",
    "    def evaluate(self, data, y_true):\n",
    "        # Performance\n",
    "        performance_cache = {}\n",
    "\n",
    "        # Get prediction\n",
    "        y_pred = self.model.predict(data)\n",
    "\n",
    "        for metric in self.metrics:\n",
    "            performance_cache[metric] = get_feval(metric)(y_true, y_pred)\n",
    "\n",
    "        return performance_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_metrics = ['recall_score', 'roc_auc_score']\n",
    "ml_eval = ModelPerformance(model, ml_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wchen/projects/adtrack_ctr/venv/lib/python3.7/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "performance_metrics = ml_eval.evaluate(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model achieved score 0.5279252262802482 of recall_score.\n",
      "Trained model achieved score 0.7577171366768948 of roc_auc_score.\n"
     ]
    }
   ],
   "source": [
    "for metric_name, metric_val in performance_metrics.items():\n",
    "    print(f\"Trained model achieved score {metric_val} of {metric_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
